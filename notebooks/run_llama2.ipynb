{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=\"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\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\",use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(user_prompt):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    # system_prompt = \"You are a helpful, respectful and honest assistant. \"\n",
    "    system_prompt = \"Answer in the json format of {\\\"Answer\\\": }.\"\n",
    "    # system_prompt = \"\"\n",
    "\n",
    "    prompt = B_INST + B_SYS + system_prompt +E_SYS + user_prompt + E_INST\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    # Generate a response from the model\n",
    "    output = model.generate(input_ids, max_new_tokens=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    # Decode and print the generated response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"What is 2+2? \"+ \"Answer in the json format of {\\\"Answer\\\": }.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run geomlama\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"../data/geomlama/inputs_lang_country.json\", 'r', encoding='utf-8') as json_file:\n",
    "    inputs_lang_country = json.load(json_file)\n",
    "\n",
    "\n",
    "lang_country_mapping = dict(zip(['zh', 'en', 'hi', 'fa', 'sw'], ['china', 'us', 'india', 'iran', 'kenya']))\n",
    "\n",
    "results = {}\n",
    "for lang in inputs_lang_country:\n",
    "    results[lang] = {}\n",
    "    for country in inputs_lang_country[lang]:\n",
    "        if country == lang_country_mapping[lang] or lang=='en':\n",
    "            results[lang][country] = []\n",
    "            print(lang, country)\n",
    "            for sample in tqdm(inputs_lang_country[lang][country][:1]):\n",
    "                messages = sample['prompt']\n",
    "                generated = chat_completion(messages)\n",
    "                results[lang][country].append({\"prompt\": sample['prompt'], \"answer\":sample['answer'], \"generated\":generated})\n",
    "\n",
    "with open(\"../results/geomlama/llama2_results.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run candle\n",
    "with open(\"../data/candle/inputs_lang_country.json\", 'r', encoding='utf-8') as json_file:\n",
    "    inputs_lang_country = json.load(json_file)\n",
    "results = {}\n",
    "for lang in inputs_lang_country:\n",
    "    results[lang] = {}\n",
    "    for country in inputs_lang_country[lang]:\n",
    "        results[lang][country] = []\n",
    "        print(lang, country)\n",
    "        for sample in tqdm(inputs_lang_country[lang][country][:1]):\n",
    "            messages = sample['prompt']\n",
    "            generated = chat_completion(messages)\n",
    "            results[lang][country].append({\"prompt\": sample['prompt'], \"answer\":sample['answer'], \"generated\":generated})\n",
    "\n",
    "with open(\"../results/candle/llama2_results.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/genericskb/inputs_lang_country_verification.json\", 'r', encoding='utf-8') as json_file:\n",
    "    inputs_lang_country = json.load(json_file)\n",
    "results = {}\n",
    "\n",
    "for lang in inputs_lang_country:\n",
    "    results[lang] = {}\n",
    "    for country in inputs_lang_country[lang]:\n",
    "        results[lang][country] = []\n",
    "        print(lang, country)\n",
    "        for sample in tqdm(inputs_lang_country[lang][country][:500][:1]):\n",
    "            messages = sample['prompt']\n",
    "            generated = chat_completion(messages)\n",
    "            results[lang][country].append({\"prompt\": sample['prompt'], \"answer\":sample['answer'], \"generated\":generated})\n",
    "\n",
    "with open(\"../results/genericskb/llama2_verification_results.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../data/genericskb/inputs_lang_country_association.json\", 'r', encoding='utf-8') as json_file:\n",
    "    inputs_lang_country = json.load(json_file)\n",
    "results = {}\n",
    "\n",
    "for lang in inputs_lang_country:\n",
    "    results[lang] = {}\n",
    "    for country in inputs_lang_country[lang]:\n",
    "        results[lang][country] = []\n",
    "        print(lang, country)\n",
    "        for sample in tqdm(inputs_lang_country[lang][country][:500][:1]):\n",
    "            messages = sample['prompt']\n",
    "            generated = chat_completion(messages)\n",
    "            results[lang][country].append({\"prompt\": sample['prompt'], \"answer\":sample['answer'], \"generated\":generated})\n",
    "\n",
    "\n",
    "with open(\"../results/genericskb/llama2_association_results.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
